# -*- coding: utf-8 -*-
"""ICM Improved.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_HOx17GjrKYbTE2lMdoSvOeaZMkI1aAg
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

# 1. Data Augmentation for Training
train_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),        # Flip images randomly
    transforms.RandomCrop(32, padding=4),     # Random crop
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Color variation
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# 2. Only Normalize Test Data
test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# 3. Load CIFAR-10 Dataset
train_data = torchvision.datasets.CIFAR10(root='/data', train=True, transform=train_transforms, download=True)
test_data = torchvision.datasets.CIFAR10(root='/data', train=False, transform=test_transforms, download=True)

# 4. Data Loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=2)

# Class Labels
class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

print(" Data loaded successfully with augmentation applied.")

import torch.nn as nn
import torch.nn.functional as F

class ImprovedCNN(nn.Module):
    def __init__(self):
        super(ImprovedCNN, self).__init__()
        # 1st Conv Layer
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)

        # 2nd Conv Layer
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)

        # 3rd Conv Layer
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # Max Pooling
        self.pool = nn.MaxPool2d(2, 2)

        # Fully Connected Layers
        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # CIFAR10 -> 32x32 -> pooling reduces size
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 10)

        # Dropout to prevent overfitting
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = torch.flatten(x, 1)
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.fc3(x)
        return x

# Instantiate the model
net = ImprovedCNN()
print(" Improved CNN model created successfully")

import torch.optim as optim

# Define Loss Function
criterion = nn.CrossEntropyLoss()

# Define Optimizer (Adam works better than SGD for this case)
optimizer = optim.Adam(net.parameters(), lr=0.001)

# Learning Rate Scheduler (reduces LR if validation loss plateaus)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

# Training Loop
epochs = 50
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
net.to(device)

import matplotlib.pyplot as plt
import numpy as np
import os

# Initialize variables for tracking metrics
train_losses = []
val_losses = []
val_accuracies = []

best_accuracy = 0.0
best_loss = float('inf')
patience = 7
counter = 0

for epoch in range(epochs):
    net.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Evaluate on test set after each epoch
    net.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = net(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # Calculate accuracy and losses
    accuracy = 100 * correct / total
    avg_train_loss = running_loss / len(train_loader)
    avg_val_loss = val_loss / len(test_loader)

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)
    val_accuracies.append(accuracy)

    # Learning rate adjustment
    scheduler.step(avg_val_loss)

    # Print metrics for this epoch
    print(f"Epoch [{epoch+1}/{epochs}] | "
          f"Train Loss: {avg_train_loss:.4f} | "
          f"Val Loss: {avg_val_loss:.4f} | "
          f"Val Accuracy: {accuracy:.2f}%")

        # Define Google Drive save path
    best_model_path = '/content/drive/MyDrive/models/best_model.pth'
    os.makedirs(os.path.dirname(best_model_path), exist_ok=True)

    # Early stopping conditions
    if accuracy >= 95.0:
        print("Early stopping triggered (accuracy reached 95%+)")
        best_accuracy = accuracy
        torch.save(net.state_dict(), best_model_path)
        print(f"Best model saved at: {best_model_path}")
        break

    if avg_val_loss < best_loss:
        best_loss = avg_val_loss
        best_accuracy = accuracy
        torch.save(net.state_dict(), best_model_path)
        print(f"Model updated (new best loss: {best_loss:.4f}) and saved at: {best_model_path}")
        counter = 0
    else:
        counter += 1
        print(f"No improvement in validation loss ({counter}/{patience})")
        if counter >= patience:
            print("Early stopping triggered (no improvement in val loss)")
            break

print("Training Completed")

epochs_ran = range(1, len(train_losses)+1)

plt.figure(figsize=(12, 4))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(epochs_ran, train_losses, label='Training Loss')
plt.plot(epochs_ran, val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs_ran, val_accuracies, label='Validation Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.title('Validation Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# pip install onnx onnxruntime

import torch

# Instantiate the model again
net = ImprovedCNN()

# Load best saved weights
best_model_path = "/content/drive/MyDrive/models/best_model.pth"
net.load_state_dict(torch.load(best_model_path, map_location='cpu'))
net.eval()
print(" Best model loaded successfully")

# Example input (dummy data with same shape as CIFAR10 images)
example_input = torch.randn(1, 3, 32, 32)

# Convert to TorchScript
traced_script_module = torch.jit.trace(net, example_input)

# Save TorchScript model
torchscript_path = "/content/drive/MyDrive/models/model_torchscript.pt"
traced_script_module.save(torchscript_path)

print("Model exported to TorchScript format successfully")


onnx_path = "/content/drive/MyDrive/models/model_onnx.onnx"

torch.onnx.export(
    net,
    example_input,
    onnx_path,
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},
    opset_version=11
)

print("Model exported to ONNX format successfully")